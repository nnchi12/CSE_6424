{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5905a69",
   "metadata": {},
   "source": [
    "# CSE6242 - HW3 - Q1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09289981",
   "metadata": {},
   "source": [
    "Pyspark Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "139318cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "### DO NOT MODIFY THIS CELL ###\n",
    "import pyspark\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.functions import hour, when, col, date_format, to_timestamp, round, coalesce"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd9e0f8",
   "metadata": {},
   "source": [
    "Initialize PySpark Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b0c18c6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/10/15 00:41:52 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "/usr/local/lib/python3.9/dist-packages/pyspark/sql/context.py:113: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "### DO NOT MODIFY THIS CELL ###\n",
    "sc = pyspark.SparkContext(appName=\"HW3-Q1\")\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d68ae314",
   "metadata": {},
   "source": [
    "Define function for loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7e5bbdda",
   "metadata": {},
   "outputs": [],
   "source": [
    "### DO NOT MODIFY THIS CELL ###\n",
    "def load_data():\n",
    "    df = sqlContext.read.option(\"header\",True) \\\n",
    "     .csv(\"yellow_tripdata_2019-01_short.csv\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d52409d",
   "metadata": {},
   "source": [
    "### Q1.a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e43f6e00",
   "metadata": {},
   "source": [
    "Perform data casting to clean incoming dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "11f801b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(df):\n",
    "    '''\n",
    "    input: df a dataframe\n",
    "    output: df a dataframe with the all the original columns\n",
    "    '''\n",
    "    \n",
    "    # START YOUR CODE HERE ---------\n",
    "    from pyspark.sql.types import IntegerType, FloatType\n",
    "    \n",
    "    # Cast passenger_count to integer\n",
    "    df = df.withColumn(\"passenger_count\", col(\"passenger_count\").cast(IntegerType()))\n",
    "    \n",
    "    # Cast total_amount, tip_amount, trip_distance, fare_amount to float\n",
    "    float_columns = [\"total_amount\", \"tip_amount\", \"trip_distance\", \"fare_amount\"]\n",
    "    for col_name in float_columns:\n",
    "        df = df.withColumn(col_name, col(col_name).cast(FloatType()))\n",
    "    \n",
    "    # Cast tpep_pickup_datetime and tpep_dropoff_datetime to timestamp\n",
    "    df = df.withColumn(\"tpep_pickup_datetime\", to_timestamp(col(\"tpep_pickup_datetime\"), \"yyyy-MM-dd HH:mm:ss\"))\n",
    "    df = df.withColumn(\"tpep_dropoff_datetime\", to_timestamp(col(\"tpep_dropoff_datetime\"), \"yyyy-MM-dd HH:mm:ss\"))\n",
    "\n",
    "    # END YOUR CODE HERE -----------\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f565d0",
   "metadata": {},
   "source": [
    "### Q1.b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b4f712",
   "metadata": {},
   "source": [
    "Find rate per person for based on how many passengers travel between pickup and dropoff locations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e115152",
   "metadata": {},
   "outputs": [],
   "source": [
    "def common_pair(df):\n",
    "    '''\n",
    "    input: df a dataframe\n",
    "    output: df a dataframe with following columns:\n",
    "            - PULocationID\n",
    "            - DOLocationID\n",
    "            - passenger_count\n",
    "            - per_person_rate\n",
    "            \n",
    "    per_person_rate is the total_amount per person for a given pair.\n",
    "    \n",
    "    '''\n",
    "    from pyspark.sql.functions import col, sum\n",
    "    from pyspark.sql.window import Window\n",
    "    from pyspark.sql import SparkSession\n",
    "    from pyspark.sql.functions import rank\t\n",
    "    \n",
    "    # START YOUR CODE HERE ---------\n",
    "    spark = SparkSession.builder.appName('Spark rank()').getOrCreate()\n",
    "\n",
    "    # Filter out trips with the same pickup and drop-off location\n",
    "    df = df.filter(col(\"PULocationID\") != col(\"DOLocationID\"))\n",
    "\n",
    "    # Group by pickup and drop-off location pairs and sum passenger_count\n",
    "    grouped_df = df.groupBy(\"PULocationID\", \"DOLocationID\").agg(\n",
    "        sum(\"passenger_count\").alias(\"passenger_count\"),\n",
    "        sum(\"total_amount\").alias(\"total_amount\")\n",
    "    )\n",
    "\n",
    "    # Calculate per_person_rate (total_amount per passenger)\n",
    "    grouped_df = grouped_df.withColumn(\"per_person_rate\", col(\"total_amount\") / col(\"passenger_count\"))\n",
    "\n",
    "    # Create a Window specification for ranking\n",
    "    window_spec = Window.orderBy(col(\"passenger_count\").desc(), col(\"per_person_rate\").desc())\n",
    "\n",
    "    # Rank the rows based on passenger_count and per_person_rate\n",
    "    ranked_df = grouped_df.withColumn(\"rank\", rank().over(window_spec))\n",
    "\n",
    "    # Select the top 10 ranked rows\n",
    "    top_10_pairs = ranked_df.filter(col(\"rank\") <= 10).drop(\"rank\", \"total_amount\")\n",
    "\n",
    "    return top_10_pairs\n",
    "    # END YOUR CODE HERE -----------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "127574ab",
   "metadata": {},
   "source": [
    "### Q1.c"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36a8fd27",
   "metadata": {},
   "source": [
    "Find trips which trip distances generate the highest tip percentage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "376c981c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance_with_most_tip(df):\n",
    "    '''\n",
    "    input: df a dataframe\n",
    "    output: df a dataframe with following columns:\n",
    "            - trip_distance\n",
    "            - tip_percent\n",
    "            \n",
    "    trip_percent is the percent of tip out of fare_amount\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # START YOUR CODE HERE ---------\n",
    "    from pyspark.sql.functions import col, when, round, avg\n",
    "    # Filter the data for trips with fares greater than $2.00 and positive trip distances\n",
    "    filtered_df = df.filter((col(\"fare_amount\") > 2) & (col(\"trip_distance\") > 1))\n",
    "\n",
    "    # Calculate tip percent for each trip\n",
    "    filtered_df = filtered_df.withColumn(\"tip_percent\", (col(\"tip_amount\") * 100 / col(\"fare_amount\")))\n",
    "\n",
    "    # Round trip distances up to the closest mile\n",
    "    filtered_df = filtered_df.withColumn(\"trip_distance\", round(col(\"trip_distance\")).cast(\"integer\"))\n",
    "\n",
    "    # Group by rounded trip distances and calculate the average tip percent\n",
    "    result_df = filtered_df.groupBy(\"trip_distance\").agg(avg(\"tip_percent\").alias(\"tip_percent\"))\n",
    "\n",
    "    # Sort the result in descending order of tip_percent\n",
    "    result_df = result_df.orderBy(col(\"tip_percent\").desc())\n",
    "\n",
    "    # Take the top 15 trip distances with the highest tip percent\n",
    "    result_df = result_df.limit(15)\n",
    "    \n",
    "    return result_df\n",
    "    # END YOUR CODE HERE -----------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0172fe6",
   "metadata": {},
   "source": [
    "### Q1.d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4613c906",
   "metadata": {},
   "source": [
    "Determine the average speed at different times of day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7a0e65b9-11f6-4c3b-b79b-6711cfb3b6a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_with_most_traffic(df):\n",
    "    '''\n",
    "    input: df a dataframe\n",
    "    output: df a dataframe with following columns:\n",
    "            - time_of_day\n",
    "            - am_avg_speed\n",
    "            - pm_avg_speed\n",
    "            \n",
    "    trip_percent is the percent of tip out of fare_amount\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # START YOUR CODE HERE ---------\n",
    "    from pyspark.sql.functions import col, hour, when, avg, date_format\n",
    "    \n",
    "    # Extract the hour of the day from the timestamp\n",
    "    df = df.filter(col(\"trip_distance\") > 0)\n",
    "    df = df.withColumn(\"hour_of_day\", hour(col(\"tpep_pickup_datetime\")))\n",
    "    \n",
    "    # Calculate the average time (avg_time) by subtracting pickup time from dropoff time\n",
    "    df = df.withColumn(\"avg_time\", (col(\"tpep_dropoff_datetime\").cast(\"long\") - col(\"tpep_pickup_datetime\").cast(\"long\")) / 3600)  # Convert to hours\n",
    "\n",
    "    # Calculate the average speed for each hour\n",
    "    df = df.groupBy(\"hour_of_day\").agg(\n",
    "        avg(\"trip_distance\").alias(\"avg_distance\"),\n",
    "        avg(\"avg_time\").alias(\"avg_time\")\n",
    "    )\n",
    "    \n",
    "    # Calculate the average speed as distance per hour\n",
    "    df = df.withColumn(\"avg_speed\", col(\"avg_distance\") / col(\"avg_time\"))\n",
    "    \n",
    "    # Create columns for AM and PM average speeds\n",
    "    df = df.withColumn(\"time_of_day\", when(col(\"hour_of_day\") < 12, col(\"hour_of_day\")).otherwise(col(\"hour_of_day\") - 12))\n",
    "    df = df.withColumn(\"am_avg_speed\", when(col(\"hour_of_day\") < 12, col(\"avg_speed\")).otherwise(None))\n",
    "    df = df.withColumn(\"pm_avg_speed\", when(col(\"hour_of_day\") >= 12, col(\"avg_speed\")).otherwise(None))\n",
    "    \n",
    "    \n",
    "    # Select and order the final columns\n",
    "    df = df.select(\"time_of_day\", \"am_avg_speed\", \"pm_avg_speed\", \"avg_time\",\"avg_speed\").orderBy(\"time_of_day\")\n",
    "\n",
    "    \n",
    "    # END YOUR CODE HERE -----------\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34cbd7b9",
   "metadata": {},
   "source": [
    "### The below cells are for you to investigate your solutions and will not be graded\n",
    "## Ensure they are commented out prior to submitting to Gradescope to avoid errors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b73e2f-cf4e-4d9f-865b-0f8ec7761e4c",
   "metadata": {},
   "source": [
    "df = load_data()\n",
    "df = clean_data(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d16ec596-9574-449a-add9-f5a19f0c447c",
   "metadata": {},
   "source": [
    "common_pair(df).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc3dc60-9df6-4458-97fd-62acf31fe171",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "distance_with_most_tip(df).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb28e8d-4fe8-49c5-9c15-8b1b44c021d0",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "time_with_most_traffic(df).show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
